DOCUMENT TITLE: Algorithmic Injustice: How AI Reinforces Systemic Bias

SOURCE: Civil Liberties Union (CLU)

DOCUMENT TYPE: Advocacy Report

DATE: March 15, 2024

---
EXECUTIVE SUMMARY:

While promoted as objective tools, AI systems deployed in critical public sectors are amplifying and hiding historical biases. Our investigation reveals that AI used in hiring, loan applications, and criminal justice consistently produces discriminatory outcomes against marginalized communities.

Key Findings:

1.  **Hiring Algorithms:** Automated resume screeners, trained on data from existing workforces, often penalize candidates from non-traditional backgrounds and disproportionately favor male applicants. These systems learn to associate keywords and experiences from past successful (and often privileged) employees with future success, creating a cycle of exclusion.

2.  **Pre-Trial Risk Assessments:** So-called "neutral" risk assessment tools used by courts to recommend bail have been shown to be unreliable and racially biased. These tools often use proxies for race, like zip codes or education levels, which results in higher risk scores for Black and Latino defendants compared to white defendants with similar case files. The CLU calls for a moratorium on their use in pre-trial decisions.

3.  **The "Black Box" Problem:** A major challenge is the lack of transparency. The proprietary nature of many foundation models makes it impossible for the public to audit them for bias. Frameworks like the NIST AI RMF are a good first step, but their voluntary nature is insufficient. The CLU advocates for mandatory, independent third-party audits for any AI system used in public sector decision-making. Federal privacy legislation is also urgently needed to give individuals control over how their data is used to train these biased systems.
